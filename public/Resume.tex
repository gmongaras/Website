\documentclass[a4paper,10pt]{article}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=0.5in}
\usepackage{enumitem}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{multicol}
\pagenumbering{gobble}

\begin{document}

\begin{center}
    {\LARGE \textbf{Gabriel Mongaras}} \\
    \vspace{10pt}
    \begin{tabular}{r l}
        \href{mailto:gmongaras@smu.edu}{gmongaras@smu.edu} \quad & $\bullet$ \quad \href{https://www.linkedin.com/in/gmongaras}{linkedin.com/in/gmongaras} \\
        \href{mailto:gabriel@mongaras.com}{gabriel@mongaras.com} \quad & $\bullet$ \quad \href{https://gabrielm.cc/}{gabrielm.cc} \\
        512 – 659 – 5405 \quad & $\bullet$ \quad \href{https://github.com/gmongaras}{github.com/gmongaras} \\
    \end{tabular} \\
    \href{https://www.youtube.com/@gabrielmongaras}{youtube.com/@gabrielmongaras} \\
\end{center}

\noindent\hrulefill

\noindent{\textbf{\underline{Objective}:}} \vspace{1pt} \\
Software Engineer at Etched with hands-on research and internship experience at major tech companies, skilled in AI model development and prototype creation. Background includes enhancing diffusion models and optimizing deep learning algorithms at Google, Amazon, and Meta. Proficient in Python, AI/ML libraries, and cloud platforms, ready to contribute to AI-powered system development and integration. Wanting to work on AI research and large scale modeling.

% \section*{EDUCATION}
\vspace{5pt}
\noindent{\large\textbf{\underline{Education}:}} \vspace{1pt} \\
\textbf{Southern Methodist University – Lyle School of Engineering} \hfill Dallas, TX \\
Masters of Science in Computer Science \hfill Expected Grad Date: May 2026

\vspace{5pt}
\noindent\textbf{Southern Methodist University – Lyle School of Engineering} \hfill Dallas, TX \\
Bachelors of Science in Computer Science \hfill Grad Date: May 2025 \\
Bachelors of Science in Statistical Science \hfill \textbf{GPA: 3.86} \\
Bachelor of Science in Data Science \\
Bachelor of Arts in Mathematics

\vspace{5pt}
\noindent\textbf{Austin Community College} \hfill Austin, TX \\
Associates of Science in Computer Programming \hfill Grad Date: May 2021 \\
Occupational Skills Award – Computer Programming \hfill \textbf{GPA: 3.9}

\vspace{5pt}
\noindent{\large\textbf{\underline{Relevant Courses}:}} \\
Graduate Quantum Computing, Graduate Artificial Intelligence, Graduate Machine Learning 2, Graduate Algorithm Engineering, Assembly Programming, Algorithms, Calculus I, II \& III, Graduate OS and System Software, Digital Logic Design, Linear Algebra, Digital History, Discrete Computational Structures, Applied Statistics, Engineering Design, Math Modeling, Math of ML, Applied Machine Learning, Data Structures


\vspace{5pt}
\noindent{\large\textbf{\underline{Skills}:}} \vspace{1pt} \\
\textbf{Coding:} Python, C++, CUDA, C, JavaScript, SQL, PL/SQL, AWS, Linux, Arduino, ARM, Android SDK, Java, Django, Flask, HTML, CSS, Rust

\noindent\textbf{AI:} Neural Networks, Generative models, PyTorch, Machine Learning, Reinforcement Learning, NumPy, CNNs, Transformers, GANs, NEAT, Diffusion Models, Object Detection, Audio Processing, Huggingface, TensorFlow, JAX, OpenAI

\noindent\textbf{Others:} AWS, Cloud Platforms, Quantum computing, Blockchain, Eagerness To Learn







\vspace{5pt}
\noindent{\large\textbf{\underline{Experience}:}} \vspace{1pt}

\noindent\textbf{Etched}, \textit{Software Engineer}, San Jose, CA \hfill June 2025-Present
\begin{itemize}[noitemsep,topsep=0pt]
  \item Helping build the codebase that will be used for Sohu in production.
\end{itemize}

\noindent\textbf{Google}, \textit{Student Researcher}, Dallas, TX \hfill October 2024-December 2024
\begin{itemize}[noitemsep,topsep=0pt]
  \item Diffusion models are slow during inference. I researched methods to improve diffusion model inference speed performance. Some tests can be found here: \href{https://github.com/gmongaras/Token_Merging_Tests}{github.com/gmongaras/Token\_Merging\_Tests}
\end{itemize}

\noindent\textbf{Google}, \textit{Software Engineering Intern}, Seattle, WA \hfill May 2024-August 2024
\begin{itemize}[noitemsep,topsep=0pt]
  \item On the Google labs team, I researched video editing using inversion techniques.
  \item Performed a literature review search on current SOTA video editing techniques.
  \item Implemented these techniques in JAX for future researches at Google to use.
\end{itemize}

\noindent\textbf{Hotshot}, \textit{AI Engineer}, Virtual \hfill April 2024-May 2024
\begin{itemize}[noitemsep,topsep=0pt]
  \item Helped make changes to the new model which improves upon the Act 1 model.
\end{itemize}

\noindent\textbf{Amazon}, \textit{Applied science Intern}, Sunnyvale, CA \hfill May 2023-August 2023
\begin{itemize}[noitemsep,topsep=0pt]
  \item On the Amazon Alexa ESP (Echo Spatial Perception) team, worked to improve the algorithm that detects which Alexa is closest to a user after saying the wake word based on audio signals coming from all devices in a household using deep learning techniques.
  \item Researched different methods to keep the model smaller, faster, and more accurate at the same time.
  \item Looked into different types of data that can be fed into the model to improve model accuracy.
\end{itemize}

\noindent\textbf{Meta}, \textit{Intern}, Menlo Park, CA \hfill May 2022-August 2022
\begin{itemize}[noitemsep,topsep=0pt]
  \item Created a working mobile app using the Android SDK for a project assigned by Meta University.
  \item Researched and created an AI model to generate random sentences from Gaussian noise for the app.
\end{itemize}

\noindent\textbf{Southern Methodist University}, \textit{Undergraduate Research Assistant}, Dallas, TX \hfill August 2021-May 2024
% \begin{itemize}[noitemsep,topsep=0pt]
%   \item Molecules have various stable equilibrium positions. When changing from one of these states to another, the mole- cule goes through a transition state which is observed using MP3 (Moller-Plesset Perturbation Theory) which is an accurate computation, but also very expensive. However it can be approximated by THC (Tensor Hypercontraction) which reduces the computation time, but also reduces the accuracy. We correct the error with an MLP which is faster than doing the MP3 computation, but more accurate than the THC measures. Our results have shown to achieve 2 orders of magnitude better than THC in terms of the MP3 value.
% \end{itemize}
\begin{itemize}[noitemsep,topsep=0pt]
  \item Worked on a researcher project in which we use MLPs to correct the error made from THC (tensor hypercontraction) state transition energy estimations. Our results have shown to achieve 2 orders of magnitude better than THC in terms of the MP3 (Moller-Plesset Perturbation Theory) value.
\end{itemize}


\newpage

\begin{tabbing}
\hspace{4in} \= \hspace{2in} \= \kill
\textbf{ACTIVITIES:} \> \textbf{AWARDS:} \\
Artificial Intelligence Club, President \> Hunt Scholars, Hyer Society Member \\
Cybersecurity Club, Member \> Rotunda Scholars, Hilltop Scholar  \\
Computer Science Club, Member \> University Honor Role, Discovery Scholar \\
Commons Council, Member \> Accelerated Pathways Masters Degree Program \\
\end{tabbing}




\vspace{0pt}
\noindent{\large\textbf{\underline{Projects}:}} \vspace{1pt}

\noindent\textbf{Stable Diffusion 3 From Scratch} \hfill Spring  2025
\begin{itemize}[noitemsep,topsep=0pt]
  \item Built a ViT that resembles Stable Diffusion 3 and a training pipeline to train a Stable Diffusion-like model completely from scratch. No huggingface, just pure torch.
  \item Sourced, cleaned, and recaptioned all data necessary to train the model from scratch.
  \item Used 8 A100s from SMUs cluster to train a 1.2B parameter model for up to 1024x1024 resolution generation. I would love to scale this to more GPUs but I am GPU poor.
  \item Code, results, models, and data found at \href{https://github.com/gmongaras/Stable-Diffusion-3-From-Scratch}{https://github.com/gmongaras/Stable-Diffusion-3-From-Scratch}
\end{itemize}

\noindent\textbf{Senior Thesis} \hfill Fall 2023/Spring 2024
\begin{itemize}[noitemsep,topsep=0pt]
  \item Worked on a method called Cottention for making transformers linear in time and memory. Linear transformers have the goal of making the memory usage from quadratic to linear, thus saving resources.
  \item Paper found here: \href{https://arxiv.org/abs/2409.18747}{arxiv.org/abs/2409.18747}
\end{itemize}

\noindent\textbf{Diffusion Models From Scratch} \hfill Fall 2022/Spring 2023
\begin{itemize}[noitemsep,topsep=0pt]
  \item Coded a Diffusion Model from pure PyTorch that learns how to produce images given random noise from a Gaussian distribution.
  \item On top of the basic DDPM model, I improved the speed of image generation by converting the model to a DDIMs, which removes the Markov chain restriction of the basic DDPM model.
  \item Added Classifier-Free guidance to improve model FID score.
  \item \href{https://github.com/gmongaras/Diffusion_models_from_scratch}{github.com/gmongaras/Diffusion\_models\_from\_scratch}
\end{itemize}

\noindent\textbf{MetaU Capstone} \hfill Summer 2022
\begin{itemize}[noitemsep,topsep=0pt]
  \item Created an app that gave daily fortunes to users which can be shared with friends found on the app.
  \item Built a model using a Transformer WGAN to generate random fortunes from Gaussian noise.
  \item \href{https://github.com/gmongaras/MetaU_Capstone}{https://github.com/gmongaras/MetaU\_Capstone}
\end{itemize}

\noindent\textbf{YOLOX From Scratch} \hfill Spring 2022/Summer 2022
\begin{itemize}[noitemsep,topsep=0pt]
  \item Coded an AI from scratch that learns how to detect objects given an image by putting bounding boxes around objects in the image.
  \item To detect objects, the algorithm predicts three attributes: The location of a bounding box to put around an object, how confident the model is that there’s an object in that bounding box, and what object is in that bounding box.
  \item  The project can be found here: \href{https://github.com/gmongaras/YOLOX_From_Scratch}{github.com/gmongaras/YOLOX\_From\_Scratch}
  \item  Additionally, I wrote an article series explaining all the parts to this algorithm:
\href{https://gmongaras.medium.com/list/yolox-explantation-1bff11aa9911}{gmongaras.medium.com/list/yolox-explantation-1bff11aa9911}
\end{itemize}






\vspace{5pt}
\noindent{\large\textbf{\underline{Publications/Articles}:}} \vspace{1pt}


\noindent\textbf{On the Expressiveness of Softmax Attention: A Recurrent Neural Network Perspective}
\begin{itemize}[noitemsep,topsep=0pt]
  \item Developed theory for softmax attention proving that the numerator is a sum of infinite recurrent neural networks of increasing hidden state size and hypothesized that the denominator is simply a norm. Alongside our proof, we provide empirical results showing that the theory aligns with practice.
  \item Code found here: \href{https://github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective}{github.com/gmongaras/On-the-Expressiveness-of-Softmax-Attention-A-Recurrent-Neural-Network-Perspective}
  \item Paper found here: \href{https://arxiv.org/abs/2507.23632}{arxiv.org/abs/2507.23632}
\end{itemize}

\noindent\textbf{Cottention: Linear Transformers With Cosine Attention}
\begin{itemize}[noitemsep,topsep=0pt]
  \item Developed a method called “Cottention”, a linear complexity attention algorithm that has similar accuracy to classic softmax attention while being faster and more memory efficient.
  \item Code found here: \href{https://github.com/gmongaras/Cottention_Transformer}{github.com/gmongaras/Cottention\_Transformer}
  \item Paper found here: \href{https://arxiv.org/abs/2409.18747}{arxiv.org/abs/2409.18747}
  \item Published in Springer Nature \href{https://link.springer.com/book/10.1007/978-3-031-92602-0?sap-outbound-id=AD9F926E0AA16D13049BD2370EAFCAD37B0D3F1F}{link.springer.com/book/10.1007/978-3-031-92602-0}
\end{itemize}

\noindent\textbf{Diffusion Models — DDPMs, DDIMs, and Classifier Free Guidance}
\begin{itemize}[noitemsep,topsep=0pt]
  \item Wrote about the evolution of base Diffusion Models and how they work.
  \item This article has been published by Better Programming
  \item \href{https://betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869}{betterprogramming.pub/diffusion-models-ddpms-ddims-and-classifier-free-guidance-e07b297b2869}
\end{itemize}

\noindent\textbf{Coding An AI Girlfriend}
\begin{itemize}[noitemsep,topsep=0pt]
  \item Explains how I coded a virtual AI girlfriend using an assortment of AI technologies
  \item \href{https://medium.com/mlearning-ai/coding-a-virtual-ai-girlfriend-f951e648aa46}{medium.com/mlearning-ai/coding-a-virtual-ai-girlfriend-f951e648aa46}
\end{itemize}

\noindent\textbf{How Do Self-Attention Masks Work?}
\begin{itemize}[noitemsep,topsep=0pt]
  \item How do masks in the self-attention function work? This article attempts to explain how they work.
  \item This article has been published by MLearning.ai
  \item \href{https://medium.com/mlearning-ai/how-do-self-attention-masks-work-72ed9382510f}{medium.com/mlearning-ai/how-do-self-attention-masks-work-72ed9382510f}
\end{itemize}






\end{document}
